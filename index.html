<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <title>论文相关实验</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!-- 引入与数学公式有关的库 -->
    <script>MathJax = {tex: {inlineMath: [['$', '$'],['$$', '$$'], ['\\(', '\\)']]}}</script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" type="text/css" href="style.css">
    <body>
    <header>
      <h2>强化学习相关实验</h2>
    </header>
    <section>
      <nav>
        <ul>
          <li><a href="#">实验1</a></li>
          <li><a href="#">实验2</a></li>
          <li><a href="#">实验3</a></li>
        </ul>
      </nav>
      
      <article>
        <h1>不同可读性的轨迹对智能体协作的影响</h1>
        <div class="row">
        <img id="gifImage" class="float-img" src="bukedu.gif" alt="图片加载中...">
        <p>
          如左图所示，蓝色智能体(观察者)观察黑色智能体的移动轨迹，进而通过贝叶斯公式判断智能体的的目标是 $g_1$ 还是$g_2$<br>
          \(
          P(g|\xi_{s_0 \rightarrow q}) = \frac{1}{z} \frac{\exp(-C(\xi_{s_0 \rightarrow q}) - C(\xi^*_{q \rightarrow g}))}{\exp(-C(\xi^*_{s_0 \rightarrow g}))} P(g)
        \)
       <br>
        <small>(公式具体推导详见Bied与Dragan的论文以及下方的计算链接)</small> <br>
        公式如上所示，由于已经过归一化处理，我们就设定: <br>
        $P(g_1|\xi_{s_0 \rightarrow q})>0.5$ 蓝色智能体相信黑色智能体去$g_1$(观察者外框颜色:黄色)<br>
        $P(g_1|\xi_{s_0 \rightarrow q})<0.5$ 蓝色智能体相信黑色智能体去$g_2$(观察者外框颜色:黑色)<br>
        $P(g_1|\xi_{s_0 \rightarrow q})=0.5$ 蓝色智能体则随机相信智能体的下一个目标(观察者外框颜色:白色)
        </p>
        <p>通过上述规则,智能体决定自己下一步的动作策略的选择(Qlearning训练完成收敛的策略)，觉得蓝色智能体要去哪里自己就去另一个目标。<br>
        左图的轨迹可读性并不是那么好，至少前五步，蓝色智能体还在随机确定目标，整体消耗的步数17步。</p>
      </div>
      <div style="clear: both;"></div> <!-- 清除浮动影响 -->  
      <div class="row">
        <img id="gifImage2" class="float-img" src="kedu.gif" alt="图片加载中...">
        <p>如左图所示，黑色智能体所走的总步数仍为7步,但整体可读性显然更好(<a href="https://raw.githubusercontent.com/z1x1ang/Legible/master/%E5%8F%AF%E8%AF%BB%E6%80%A7%E5%85%AC%E5%BC%8F%E8%AE%A1%E7%AE%97%E8%AF%A6%E7%BB%86%E8%BF%87%E7%A8%8B.jpg">具体计算过程</a>)，蓝色智能体从第一步一开始就对黑色智能体所要到达的目标充满信心，整体消耗步数15步。</p>
        <p>通过分析可知,可读性较好的动作轨迹，能够有效减少系统的成本消耗。但同样可读性过好的轨迹(绕路),可读性较好的动作轨迹，能够有效减少系统的成本消耗。但同样可读性过好的轨迹
          (绕路)反而会增加系统总的成本消耗，所以必须要把可读性$\lambda$限制在一定的范围内，这个可以作为进一步的研究方向。
        </p>
      </div>
      <p>
        为了提升智能体动作产生轨迹的可读性，并将你已有的可读性计算方法融入上述系统，我们可以通过以下几个步骤来实现：

### 1. 定义可读性评分
首先，需要明确可读性评分的具体定义和计算方法。这个评分应该能够量化轨迹的清晰度、直观性或易理解性。例如，可读性评分可以基于以下因素：
- **动作的直观性**：简单、直接的动作序列得分更高。
- **状态的连贯性**：状态转换的逻辑性和连贯性。
- **目标导向性**：轨迹是否有效地向目标前进，减少无关的循环或迂回。

### 2. 整合可读性评分到强化学习
将可读性评分作为奖励函数的一部分或作为训练智能体时的一个优化目标。具体步骤包括：
- **修改奖励函数**：在传统的强化学习奖励机制中，将可读性评分加入到奖励函数中。例如，每一步的奖励可以是传统的奖励加上该步骤的可读性评分。
- **多目标优化**：在训练过程中，不仅优化期望的累积奖励，还要优化轨迹的可读性。这可能需要采用多目标强化学习算法，或者调整算法以平衡这两种目标。

### 3. 利用网络分割和影响力节点
利用网络中的landmark和状态团来引导智能体产生更加可读的轨迹：
- **状态团的作用**：每个状态团可以定义为一个子目标或阶段，智能体在达到每个子目标时，都应该重新评估其轨迹的可读性。
- **landmark之间的路径优化**：在计算从一个landmark到另一个landmark的路径时，优先选择可读性高的路径。

### 4. 状态团到landmark的路径选择
为了增强整个轨迹的可读性，可以特别关注从状态团到landmark的路径选择：
- **条件概率与可读性的结合**：在选择下一步动作时，不仅考虑到达landmark的概率，还要考虑这一行动的可读性评分。

### 5. 训练和评估
- **训练**：使用修改后的奖励函数训练智能体，同时监控可读性和任务完成率的表现。
- **评估**：通过独立测试集评估智能体在未见过的网格世界中的表现，特别注意评估轨迹的可读性和效率。

通过这些步骤，你可以将可读性评分有效地融合到智能体的学习过程中，不仅提高智能体的任务执行效率，还增强了其行动轨迹的直观性和易理解性。这种方法特别适用于需要智能体与人类用户交互或协作的应用场景。
      </p>
      </article>
    </section>
    <footer>
      <p>Footer</p>
    </footer>
    
    </body>
</head>
</html>